import { ChatRequest, HChat } from "../src";
import dotenv from "dotenv";

// .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
dotenv.config();
const client = new HChat({
  apiKey: process.env.HCHAT_API_KEY!,
  debug: false,
});

async function basicExample(model: string = "gpt-4.1") {
  console.log("ğŸ”· Basic Text Generation");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant.",
    content: [
      {
        role: "user",
        content: "í•œêµ­ì˜ ì „í†µ ìŒì‹ 3ê°€ì§€ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.",
      },
    ],
  };
  const response = await client.chat(request);

  console.log("\nğŸ“ Response:");
  console.log(response.choices[0].message.content);

  if (response.usage) {
    console.log("\nğŸ“Š Token Usage:");
    console.log(`  - Prompt tokens: ${response.usage.prompt_tokens}`);
    console.log(`  - Completion tokens: ${response.usage.completion_tokens}`);
    console.log(`  - Total tokens: ${response.usage.total_tokens}`);
  }

  console.log("\nâœ… Complete!");
}

async function streamExample(model: string = "gpt-4.1") {
  console.log("ğŸ”· Stream Text Generation");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant.",
    content: [
      {
        role: "user",
        content: "í•œêµ­ì˜ ì „í†µ ìŒì‹ 3ê°€ì§€ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.",
      },
    ],
  };

  let response = "";
  for await (const chunk of client.stream(request)) {
    process.stdout.write(chunk);
    response += chunk;
  }

  console.log("\n\nâœ… Complete!");
}

async function chatThinkingExample(model: string = "gpt-4.1") {
  console.log("ğŸ¤” Thinking Mode (Non-stream)");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant.",
    content: [
      {
        role: "user",
        content: `ë¬¸ì œ: 1ë¶€í„° 100ê¹Œì§€ì˜ ìì—°ìˆ˜ ì¤‘ì—ì„œ 3ì˜ ë°°ìˆ˜ì˜ í•©ì„ êµ¬í•˜ì„¸ìš”. 
ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ë©° í’€ì´í•´ì£¼ì„¸ìš”.`,
      },
    ],
    thinking: true,
  };
  const response = await client.chat(request);

  // Thinking processë¥¼ ë¨¼ì € í‘œì‹œ
  if (response.choices[0].message.thinking) {
    console.log("\nğŸ¤” Thinking Process:");
    console.log(response.choices[0].message.thinking);
  }

  console.log("\nğŸ“ Response:");
  console.log(response.choices[0].message.content);

  if (response.usage) {
    console.log("\nğŸ“Š Token Usage:");
    console.log(`  - Prompt tokens: ${response.usage.prompt_tokens}`);
    console.log(`  - Completion tokens: ${response.usage.completion_tokens}`);
    console.log(`  - Total tokens: ${response.usage.total_tokens}`);
    if (response.usage.thinking_tokens) {
      console.log(`  - Thinking tokens: ${response.usage.thinking_tokens}`);
    }
  }

  console.log("\nâœ… Complete!");
}

async function streamThinkingExample(model: string = "gpt-4.1") {
  console.log("ğŸ¤” Thinking Mode (Stream)");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant.",
    content: [
      {
        role: "user",
        content: `ë¬¸ì œ: 1ë¶€í„° 100ê¹Œì§€ì˜ ìì—°ìˆ˜ ì¤‘ì—ì„œ 3ì˜ ë°°ìˆ˜ì˜ í•©ì„ êµ¬í•˜ì„¸ìš”.`,
      },
    ],
    thinking: true,
  };

  let response = "";
  for await (const chunk of client.stream(request)) {
    process.stdout.write(chunk);
    response += chunk;
  }

  console.log("\n\nâœ… Complete!");
}

async function chatWithToolsExample(model: string = "gpt-4.1") {
  console.log("ğŸ”§ Tool Use Example (Non-stream)");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant with access to tools.",
    content: [
      {
        role: "user",
        content: `ì˜¤ëŠ˜ ì„œìš¸ì˜ ë‚ ì”¨ëŠ” ì–´ë–¤ê°€ìš”?`,
      },
    ],
    tools: [
      {
        type: "function",
        function: {
          name: "get_weather",
          description: "Get the current weather for a location",
          parameters: {
            type: "object",
            properties: {
              location: {
                type: "string",
                description: "The city name",
              },
              unit: {
                type: "string",
                enum: ["celsius", "fahrenheit"],
                description: "Temperature unit",
              },
            },
            required: ["location"],
          },
        },
      },
    ],
  };

  const response = await client.chat(request);

  console.log("\nğŸ“ Response:");
  const message = response.choices[0].message;

  if (message.tool_calls && message.tool_calls.length > 0) {
    console.log("\nğŸ”§ Tool Calls:");
    for (const toolCall of message.tool_calls) {
      console.log(`  - Function: ${toolCall.function.name}`);
      console.log(`    Arguments: ${toolCall.function.arguments}`);
    }
  } else {
    console.log(message.content);
  }

  if (response.usage) {
    console.log("\nğŸ“Š Token Usage:");
    console.log(`  - Prompt tokens: ${response.usage.prompt_tokens}`);
    console.log(`  - Completion tokens: ${response.usage.completion_tokens}`);
    console.log(`  - Total tokens: ${response.usage.total_tokens}`);
  }

  console.log("\nâœ… Complete!");
}

async function runExamples() {
  // ëª¨ë¸ ì„ íƒ
  const model = "gpt-4.1";
  // const model = "gpt-4o";
  // const model = "gemini-2.5-flash";
  // const model = "gemini-2.5-pro";
  // const model = "claude-sonnet-4";
  // const model = "claude-opus-4";

  console.log(`\nğŸš€ Testing with model: ${model}\n`);
  console.log("=".repeat(50));

  try {
    // 1. ê¸°ë³¸ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¼)
    await basicExample(model);
    console.log("\n" + "=".repeat(50) + "\n");

    // 2. ìŠ¤íŠ¸ë¦¼ ì±„íŒ…
    await streamExample(model);
    console.log("\n" + "=".repeat(50) + "\n");

    // 3. Thinking ëª¨ë“œ (ë¹„ìŠ¤íŠ¸ë¦¼)
    await chatThinkingExample(model);
    console.log("\n" + "=".repeat(50) + "\n");

    // 4. Thinking ëª¨ë“œ (ìŠ¤íŠ¸ë¦¼)
    await streamThinkingExample(model);
    console.log("\n" + "=".repeat(50) + "\n");

    // 5. Tool ì‚¬ìš© ì˜ˆì œ
    await chatWithToolsExample(model);
  } catch (error: any) {
    // ê¹¨ë—í•œ ì—ëŸ¬ í‘œì‹œ
    console.error("\nâŒ ì—ëŸ¬ ë°œìƒ:");
    console.error(error.message);

    // ë””ë²„ê·¸ ëª¨ë“œì¼ ë•Œë§Œ ìƒì„¸ ì •ë³´ í‘œì‹œ
    if (error.originalError && process.env.DEBUG === "true") {
      console.error("\nğŸ” ìƒì„¸ ì—ëŸ¬:", error.originalError);
    }
  }
}

// ì‹¤í–‰
runExamples().catch(console.error);
