import { ChatRequest, HChat } from "../src";

async function basicExample(model: string = "gpt-4.1") {
  const client = new HChat({
    apiKey: process.env.HCHAT_API_KEY!,
    debug: true,
  });

  console.log("ğŸ”· Basic Text Generation");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant.",
    content: [
      {
        role: "user",
        content: "í•œêµ­ì˜ ì „í†µ ìŒì‹ 3ê°€ì§€ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.",
      },
    ],
  };

  let response = "";
  for await (const chunk of client.stream(request)) {
    process.stdout.write(chunk);
    response += chunk;
  }

  console.log("\n\nâœ… Complete!");
}

async function thinkingExample(model: string = "gpt-4.1") {
  const client = new HChat({
    apiKey: process.env.HCHAT_API_KEY!,
    debug: true,
  });

  console.log("ğŸ”· Thinking Text Generation");

  const request: ChatRequest = {
    model: model,
    system: "You are a helpful assistant.",
    content: [
      {
        role: "user",
        content: `í•œêµ­ì˜ ì „í†µ ìŒì‹ 3ê°€ì§€ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.`,
      },
    ],
    thinking: true,
  };

  let response = "";
  for await (const chunk of client.stream(request)) {
    process.stdout.write(chunk);
    response += chunk;
  }

  console.log("\n\nâœ… Complete!");
}

// Run
// const model =  "gpt-4.1";
const model = "gemini-2.5-flash";
// const model = "gemini-2.5-pro";
// const model = "claude-sonnet-4";
// basicExample(model).catch(console.error);
thinkingExample(model).catch(console.error);
