// src/client.ts
import {
  ChatRequest,
  ProviderChatRequest,
  HChatConfig,
  MCPManager,
  ChatCompletion,
  RequestMessage,
} from "./types";
import { BaseProvider } from "./providers/base.provider";
import { OpenAIProvider } from "./providers/openai.provider";
import { ClaudeProvider } from "./providers/claude.provider";
import { GeminiProvider } from "./providers/gemini.provider";
import { getModelProvider } from "./utils/model";
import { parseFeatureError } from "./utils/feature-support";

export class HChat {
  private providers: Map<string, BaseProvider> = new Map();
  private mcpManager?: MCPManager;

  constructor(private config: HChatConfig, mcpManager?: MCPManager) {
    this.config = config;
    this.mcpManager = mcpManager;
    this.initProviders();
  }

  private initProviders() {
    this.providers.set("openai", new OpenAIProvider(this.config));
    this.providers.set("claude", new ClaudeProvider(this.config));
    this.providers.set("gemini", new GeminiProvider(this.config));
  }

  async chat(request: ChatRequest): Promise<ChatCompletion> {
    const providerName = getModelProvider(request.model);
    const provider = this.providers.get(providerName);

    if (!provider) {
      throw new Error(`Provider not found for model: ${request.model}`);
    }
    if (request.tools === undefined) {
      request.tools = [];
    }

    // ê¸°ëŠ¥ ì§€ì› ì—¬ë¶€ëŠ” ì„œë²„ê°€ íŒë‹¨í•˜ë„ë¡ í•¨
    // í´ë¼ì´ì–¸íŠ¸ëŠ” ì—ëŸ¬ ë°œìƒ ì‹œ ì ì ˆí•œ ê°€ì´ë“œë§Œ ì œê³µ

    // contentê°€ stringì¸ ê²½ìš° RequestMessage[]ë¡œ ë³€í™˜
    const normalizedContent = this.normalizeContent(request.content);

    const providerRequest: ProviderChatRequest = {
      model: request.model,
      system: request.system,
      content: normalizedContent, // ì´ì œ í™•ì‹¤íˆ RequestMessage[] íƒ€ì…
      stream: request.stream ? request.stream : false, // ê¸°ë³¸ false
      thinking: request.thinking ? request.thinking : false, // ê¸°ë³¸ false
      max_tokens: request.max_tokens ? request.max_tokens : 4096,
      temperature: request.temperature ? request.temperature : 0.7,
      tools: request.tools.length > 0 ? request.tools : undefined,
      advanced: request.advanced ? request.advanced : undefined,
    };

    try {
      return await provider.chat(providerRequest);
    } catch (error) {
      this.log(`Error in chat: ${error}`); // ê¸°ëŠ¥ ê´€ë ¨ ì—ëŸ¬ì¸ ê²½ìš° ë” ëª…í™•í•œ ë©”ì‹œì§€ ì œê³µ
      const featureError = parseFeatureError(error);
      if (featureError) {
        const enhancedError = new Error(
          `${featureError.message}\nğŸ’¡ ${featureError.suggestion}`
        );
        // ì›ë³¸ ì—ëŸ¬ëŠ” stackì—ë§Œ ë‚¨ê¸°ê³  ì‚¬ìš©ìì—ê²ŒëŠ” ë³´ì—¬ì£¼ì§€ ì•ŠìŒ
        if (this.config.debug) {
          this.log("Original error:", error);
        }
        (enhancedError as any).code = featureError.code;
        (enhancedError as any).feature = featureError.feature;
        (enhancedError as any).model = featureError.model;
        (enhancedError as any).originalError = error;
        throw enhancedError;
      }

      throw error;
    }
  }

  /**
   * í†µí•© ìŠ¤íŠ¸ë¦¼ ì¸í„°í˜ì´ìŠ¤
   */
  async *stream(request: ChatRequest): AsyncIterable<string> {
    // 1. Provider ì„ íƒ
    const providerName = getModelProvider(request.model);
    const provider = this.providers.get(providerName);

    if (!provider) {
      throw new Error(`Provider not found for model: ${request.model}`);
    }
    if (request.tools === undefined) {
      request.tools = [];
    }

    // ê¸°ëŠ¥ ì§€ì› ì—¬ë¶€ëŠ” ì„œë²„ê°€ íŒë‹¨í•˜ë„ë¡ í•¨

    // contentê°€ stringì¸ ê²½ìš° RequestMessage[]ë¡œ ë³€í™˜
    const normalizedContent = this.normalizeContent(request.content);

    // 2. Provider ìš”ì²­ ìƒì„±
    const providerRequest: ProviderChatRequest = {
      model: request.model,
      system: request.system,
      content: normalizedContent, // ì´ì œ í™•ì‹¤íˆ RequestMessage[] íƒ€ì…
      stream: request.stream ? request.stream : false, // ê¸°ë³¸ false
      thinking: request.thinking ? request.thinking : false, // ê¸°ë³¸ false
      max_tokens: request.max_tokens ? request.max_tokens : 4096,
      temperature: request.temperature ? request.temperature : 0.7,
      tools: request.tools.length > 0 ? request.tools : undefined,
      advanced: request.advanced ? request.advanced : undefined,
    };

    // ì¶”í›„ chatì´ëƒ streamì´ëƒì— ë”°ë¼ ë¶„ê¸° ìˆ˜í–‰

    try {
      for await (const chunk of provider.stream(providerRequest)) {
        // OpenAI SDKì˜ chunk í˜•ì‹ì— ë§ê²Œ ì²˜ë¦¬
        if (chunk.choices?.[0]?.delta?.content) {
          yield chunk.choices[0].delta.content;
        }

        // Tool call ì²˜ë¦¬
        if (chunk.choices?.[0]?.delta?.tool_calls) {
          if (this.config.debug) {
            console.log(
              "Tool call detected:",
              chunk.choices[0].delta.tool_calls
            );
          }
        }
      }
    } catch (error) {
      this.log(`Error in stream: ${error}`); // ê¸°ëŠ¥ ê´€ë ¨ ì—ëŸ¬ì¸ ê²½ìš° ë” ëª…í™•í•œ ë©”ì‹œì§€ ì œê³µ
      const featureError = parseFeatureError(error);
      if (featureError) {
        const enhancedError = new Error(
          `${featureError.message}\nğŸ’¡ ${featureError.suggestion}`
        );
        // ì›ë³¸ ì—ëŸ¬ëŠ” stackì—ë§Œ ë‚¨ê¸°ê³  ì‚¬ìš©ìì—ê²ŒëŠ” ë³´ì—¬ì£¼ì§€ ì•ŠìŒ
        if (this.config.debug) {
          this.log("Original error:", error);
        }
        (enhancedError as any).code = featureError.code;
        (enhancedError as any).feature = featureError.feature;
        (enhancedError as any).model = featureError.model;
        (enhancedError as any).originalError = error;
        throw enhancedError;
      }

      throw error;
    }
  }
  private log(...args: any[]) {
    if (this.config.debug) {
      console.log("[HChat]", ...args);
    }
  }
  /**
   * contentë¥¼ RequestMessage[] í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
   */
  private normalizeContent(
    content: string | RequestMessage[]
  ): RequestMessage[] {
    if (typeof content === "string") {
      return [
        {
          role: "user",
          content: content,
        },
      ];
    }
    return content;
  }
}
